{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import PIL\n",
    "from torch import nn\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from landmark_crop import FaceCropper\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, df, face_cropper):\n",
    "        self.df = df\n",
    "        self.normalize = transforms.Normalize(mean = [0.485,0.456,0.406,0.485,0.456,0.406],std = [0.229,0.224,0.225, 0.229,0.224,0.225])\n",
    "\n",
    "        self.face_cropper = face_cropper\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        file = self.df.iloc[ix]\n",
    "        age = torch.tensor(file.age / 116)\n",
    "        gender = torch.tensor(file.gender).view(-1)\n",
    "        eth = torch.tensor(file.ethnicity)\n",
    "        im = file.pixels\n",
    "        im = np.array(Image.fromarray(im).convert(\"RGB\"))\n",
    "        print(im.shape)\n",
    "\n",
    "        # Use the FaceCropper to crop faces before normalization\n",
    "        cropped_faces = self.face_cropper.crop_faces_and_concat(im, mask=False)\n",
    "        print(cropped_faces.shape)\n",
    "\n",
    "        # Assuming you want to use the first cropped face (you can modify this as needed)\n",
    "        im = cropped_faces\n",
    "\n",
    "        # im = cv.resize(im, (224, 224))\n",
    "        # im = torch.tensor(im).permute(2, 1, 2)\n",
    "        # im = torch.tensor(im).permute(2,0,1)\n",
    "        im = self.normalize(im/255)\n",
    "\n",
    "\n",
    "        return im.float().to(device), age.to(device), gender.float().to(device), eth.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niran/miniconda3/envs/deepface/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/niran/miniconda3/envs/deepface/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/niran/miniconda3/envs/deepface/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load VGG16 backbone with pretrained weights\n",
    "        self.vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "        # Modify input layer to accept 6 channels\n",
    "        self.vgg16.features[0] = nn.Conv2d(6, 64, kernel_size=(3, 3), padding=(1, 1), bias=False)\n",
    "\n",
    "        # Freeze backbone layers for transfer learning\n",
    "        for param in self.vgg16.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define output layers\n",
    "        self.age_classifier = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.gender_classifier = nn.Sequential(\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.eth_classifier = nn.Sequential(\n",
    "            nn.Linear(64, 5),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg16.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        # Add a new layer to reduce features to 64\n",
    "        x = nn.Linear(25088, 64)(x)\n",
    "\n",
    "        age_output = self.age_classifier(x)\n",
    "        gender_output = self.gender_classifier(x)\n",
    "        eth_output = self.eth_classifier(x)\n",
    "\n",
    "        return age_output, gender_output, eth_output\n",
    "    \n",
    "model = CustomModel()\n",
    "# Input tensor with shape (batch_size, channels, height, width)\n",
    "input_tensor = torch.randn(32, 6, 244, 244)\n",
    "\n",
    "# Pass input through model\n",
    "age_output, gender_output, eth_output = model(input_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
